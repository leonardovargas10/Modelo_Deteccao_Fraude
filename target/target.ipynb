{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color: blue; font-size: 34px; font-weight: bold;'> Modelo de Detecção de Fraude\n",
    "</h1>\n",
    "<p style='font-size: 18px; line-height: 2; margin: 0px 0px; text-align: justify; text-indent: 0px;'>    \n",
    "<i> </i> \n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red' style='font-size: 40px;'> Library   </font>\n",
    "<hr style='border: 2px solid red;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Bibliotecas de Análise de Dados\n",
    "import pandas as pd \n",
    "import builtins as builtins\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from IPython.display import display, Image\n",
    "from tabulate import tabulate\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Bibliotecas de Manipulação de Tempo\n",
    "from datetime import datetime, date\n",
    "\n",
    "## Bibliotecas de Modelagem Matemática e Estatística\n",
    "import numpy as np\n",
    "import scipy as sp \n",
    "import scipy.stats as stats\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import normaltest, ttest_ind, ttest_rel, mannwhitneyu, wilcoxon, kruskal, uniform, chi2_contingency\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from numpy import interp\n",
    "import random\n",
    "\n",
    "# Bibliotecas de Seleção de Modelos\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Bibliotecas de Pré-Processamento e Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Bibliotecas de Modelos de Machine Learning\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Bibliotecas de Métricas de Machine Learning\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, precision_score, recall_score, precision_recall_curve, average_precision_score, f1_score, log_loss, brier_score_loss, confusion_matrix, silhouette_score\n",
    "\n",
    "\n",
    "# Parâmetros de Otimização\n",
    "import random\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = '14'\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "pd.set_option('display.max_rows', 100) \n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x) # Tira os números do formato de Notação Científica\n",
    "np.set_printoptions(suppress=True) # Tira os números do formato de Notação Científica em Numpy Arrays\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Retira Future Warnings\n",
    "# from pyspark.sql import SparkSession, Row \n",
    "# from pyspark.sql.functions import *\n",
    "# import pyspark.sql.functions as F\n",
    " \n",
    "\n",
    "# # Spark Session\n",
    "# spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plota_barras(lista_variaveis, df, titulo, rotation=0):        \n",
    "    k = 0\n",
    "    # Ordena os dados para garantir que as labels correspondam corretamente às barras\n",
    "    df_sorted = df[lista_variaveis[k]].value_counts().index\n",
    "    ax = sns.countplot(x=lista_variaveis[k], data=df, order=df_sorted, color='#1FB3E5')\n",
    "    \n",
    "    ax.set_title(f'{titulo}')\n",
    "    ax.set_xlabel(f'{lista_variaveis[k]}', fontsize=14)\n",
    "    ax.set_ylabel('Quantidade', fontsize=14)\n",
    "    \n",
    "    # Calcular o total para obter os percentuais\n",
    "    total = sum([p.get_height() for p in ax.patches])\n",
    "    \n",
    "    sizes = []\n",
    "    for bar in ax.patches:\n",
    "        height = bar.get_height()\n",
    "        sizes.append(height)\n",
    "        ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                height,\n",
    "                f'{builtins.round((height/total)*100, 2)}%',\n",
    "                ha='center',\n",
    "                fontsize=12\n",
    "        )\n",
    "    \n",
    "    ax.set_ylim(0, max(sizes) * 1.1)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation, ha='right', fontsize=10)\n",
    "    ax.set_yticklabels(['{:,.0f}'.format(y) for y in ax.get_yticks()], fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plota_grafico_linhas(df, x, y, nao_calcula_media, title):\n",
    "\n",
    "    if nao_calcula_media:\n",
    "        # Criando o gráfico de linha\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df[x], df[y], marker='o', linestyle='-', color='#1FB3E5')\n",
    "\n",
    "        # Adicionando títulos e rótulos aos eixos\n",
    "        plt.title(title)\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "\n",
    "        for i, txt in enumerate(df[y]):\n",
    "            plt.annotate(f'{txt:.1f}', (df[x][i], df[y][i]), textcoords=\"offset points\", xytext=(0,1), ha='center')\n",
    "\n",
    "        # Exibindo o gráfico\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        media = df[y].mean()\n",
    "        # Criando o gráfico de linha\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df[x], df[y], marker='o', linestyle='-', color='#1FB3E5')\n",
    "\n",
    "        # Adicionando linha da média\n",
    "        plt.axhline(y=media, color='r', linestyle='--', linewidth=1, label=f'Média: {media:.2f}')\n",
    "        plt.legend()\n",
    "\n",
    "        # Adicionando títulos e rótulos aos eixos\n",
    "        plt.title(title)\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "\n",
    "        for i, txt in enumerate(df[y]):\n",
    "            plt.annotate(f'{txt:.1f}', (df[x][i], df[y][i]), textcoords=\"offset points\", xytext=(0,1), ha='center')\n",
    "\n",
    "        # Exibindo o gráfico\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisa_distribuicao_via_percentis(df, variaveis):\n",
    "    def sublinha_percentis(s):\n",
    "        is_1_percentile = s.name == '1%'\n",
    "        is_99_8_percentile = s.name == '99%'\n",
    "        if is_1_percentile or is_99_8_percentile:\n",
    "            return ['background-color: blue'] * len(s)\n",
    "        else:\n",
    "            return [''] * len(s)\n",
    "\n",
    "    percentis = df[variaveis].describe(percentiles = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]).style.apply(sublinha_percentis, axis=1)    \n",
    "\n",
    "    return percentis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compara_medias_amostras(df, variaveis_continuas, target):  \n",
    "    num_variaveis = len(variaveis_continuas)\n",
    "    num_pares = (num_variaveis + 1) // 2  # Número de pares de variáveis para subplots\n",
    "    fig, axes = plt.subplots(num_pares, 2, figsize=(14, 4 * num_pares))\n",
    "\n",
    "    # Ajusta para o caso onde há apenas uma variável\n",
    "    if num_pares == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    \n",
    "    for i in range(num_pares):\n",
    "        if 2 * i < num_variaveis:\n",
    "            variavel1 = variaveis_continuas[2 * i]\n",
    "            percentis1 = df[variavel1].describe(percentiles=[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99])\n",
    "            p1_1 = percentis1['1%']\n",
    "            p99_1 = percentis1['99%']\n",
    "            df_raw1 = df.loc[(df[variavel1] > p1_1) & (df[variavel1] < p99_1)].copy()\n",
    "            df_com_bad1 = df_raw1.loc[df_raw1[target] == 1]\n",
    "            df_sem_bad1 = df_raw1.loc[df_raw1[target] == 0]\n",
    "            \n",
    "            medias_amostrais_com_bad1 = []\n",
    "            medias_amostrais_sem_bad1 = []\n",
    "            \n",
    "            for j in range(5000):\n",
    "                amostra_bad1 = random.choices(df_com_bad1[variavel1].values, k=1000)\n",
    "                media_amostra_bad1 = np.mean(amostra_bad1)\n",
    "                medias_amostrais_com_bad1.append(media_amostra_bad1)\n",
    "\n",
    "                amostra_sem_bad1 = random.choices(df_sem_bad1[variavel1].values, k=1000)\n",
    "                media_amostra_sem_bad1 = np.mean(amostra_sem_bad1)\n",
    "                medias_amostrais_sem_bad1.append(media_amostra_sem_bad1)\n",
    "\n",
    "            ax_hist1 = axes[i, 0]\n",
    "            ax_hist1.hist(medias_amostrais_com_bad1, bins=30, alpha=0.5, label='bad', linewidth=5, color=\"red\")\n",
    "            ax_hist1.hist(medias_amostrais_sem_bad1, bins=30, alpha=0.5, label='Sem bad', linewidth=5, color=\"green\")\n",
    "            ax_hist1.legend(loc='upper right')\n",
    "            ax_hist1.set_xlabel('Valores')\n",
    "            ax_hist1.set_ylabel('Frequência')\n",
    "            ax_hist1.set_title(f'Distribuição das Médias Amostrais de \"{variavel1}\" ')\n",
    "            ax_hist1.grid(True)\n",
    "        \n",
    "        if 2 * i + 1 < num_variaveis:\n",
    "            variavel2 = variaveis_continuas[2 * i + 1]\n",
    "            percentis2 = df[variavel2].describe(percentiles=[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99])\n",
    "            p1_2 = percentis2['1%']\n",
    "            p99_2 = percentis2['99%']\n",
    "            df_raw2 = df.loc[(df[variavel2] > p1_2) & (df[variavel2] < p99_2)].copy()\n",
    "            df_com_bad2 = df_raw2.loc[df_raw2[target] == 1]\n",
    "            df_sem_bad2 = df_raw2.loc[df_raw2[target] == 0]\n",
    "            \n",
    "            medias_amostrais_com_bad2 = []\n",
    "            medias_amostrais_sem_bad2 = []\n",
    "            \n",
    "            for j in range(5000):\n",
    "                amostra_bad2 = random.choices(df_com_bad2[variavel2].values, k=1000)\n",
    "                media_amostra_bad2 = np.mean(amostra_bad2)\n",
    "                medias_amostrais_com_bad2.append(media_amostra_bad2)\n",
    "\n",
    "                amostra_sem_bad2 = random.choices(df_sem_bad2[variavel2].values, k=1000)\n",
    "                media_amostra_sem_bad2 = np.mean(amostra_sem_bad2)\n",
    "                medias_amostrais_sem_bad2.append(media_amostra_sem_bad2)\n",
    "\n",
    "            ax_hist2 = axes[i, 1]\n",
    "            ax_hist2.hist(medias_amostrais_com_bad2, bins=30, alpha=0.5, label='bad', linewidth=5, color=\"red\")\n",
    "            ax_hist2.hist(medias_amostrais_sem_bad2, bins=30, alpha=0.5, label='Sem bad', linewidth=5, color=\"green\")\n",
    "            ax_hist2.legend(loc='upper right')\n",
    "            ax_hist2.set_xlabel('Valores')\n",
    "            ax_hist2.set_ylabel('Frequência')\n",
    "            ax_hist2.set_title(f'Distribuição das Médias Amostrais de \"{variavel2}\" ')\n",
    "            ax_hist2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_histogram_and_boxplot(df, variavel, target):\n",
    "    \"\"\"\n",
    "    Cria dois gráficos lado a lado:\n",
    "    - Esquerda: Histogramas da variável separados pela target, representados como linhas (sem preenchimento).\n",
    "    - Direita: Boxplots da variável separados pela target.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - df: DataFrame contendo os dados.\n",
    "    - variavel: Nome da variável a ser analisada.\n",
    "    - target: Nome da variável target que separa os grupos.\n",
    "    \"\"\"\n",
    "    # Configuração do tamanho da figura\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "    # Gráfico 1: Histogramas como linhas\n",
    "    for label, color in zip(df[target].unique(), ['blue', 'orange']):\n",
    "        subset = df[df[target] == label]\n",
    "        axes[0].hist(\n",
    "            subset[variavel], \n",
    "            bins=30, \n",
    "            color=color, \n",
    "            label=f'{label}', \n",
    "            histtype='step',  # Define o histograma como linha\n",
    "            density=True,    # Normaliza para densidade\n",
    "            linewidth=2       # Espessura da linha\n",
    "        )\n",
    "    axes[0].set_title(f'Distribuição de Bons e Maus para {variavel}', fontsize=12)\n",
    "    axes[0].set_xlabel(variavel)\n",
    "    axes[0].set_ylabel('Densidade')\n",
    "    axes[0].legend(title=target)\n",
    "    \n",
    "    # Gráfico 2: Boxplots\n",
    "    sns.boxplot(x=target, y=variavel, data=df, ax=axes[1], palette=['blue', 'orange'])\n",
    "    axes[1].set_title(f'Boxplot de Bons e Maus para {variavel}', fontsize=12)\n",
    "    axes[1].set_xlabel(target)\n",
    "    axes[1].set_ylabel(variavel)\n",
    "\n",
    "    # Ajuste da disposição dos gráficos\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_deciles(df, variavel_continua):\n",
    "    # Calcula os limites dos deciles\n",
    "    decile_limits = [i / 10 for i in range(11)]  # [0.0, 0.1, 0.2, ..., 1.0]\n",
    "    \n",
    "    # Aplica a função qcut para transformar a variável em deciles\n",
    "    deciles = pd.qcut(df[variavel_continua], q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    return deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe(df, feature, target):\n",
    "    mau = df.loc[df[target] == 1].groupby(feature, as_index = False)[target].count().rename({target:'mau'}, axis = 1)\n",
    "    sem_mau = df.loc[df[target] == 0].groupby(feature, as_index = False)[target].count().rename({target:'sem_mau'}, axis = 1)\n",
    "\n",
    "    woe = mau.merge(sem_mau, on = feature, how = 'left')\n",
    "    woe['percent_mau'] = woe['mau']/woe['mau'].sum()\n",
    "    woe['percent_sem_mau'] = woe['sem_mau']/woe['sem_mau'].sum()\n",
    "    woe['woe'] = round(np.log(woe['percent_mau']/woe['percent_sem_mau']), 3)\n",
    "    woe.sort_values(by = 'woe', ascending = True, inplace = True)\n",
    "    \n",
    "    weight_of_evidence = woe['woe'].unique()\n",
    "\n",
    "\n",
    "    x = list(woe[feature])\n",
    "    y = list(woe['woe'])\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, y, marker='o', linestyle='--', linewidth=2, color='#1FB3E5')\n",
    "\n",
    "    for label, value in zip(x, y):\n",
    "        plt.text(x=label, y=value, s=str(value), fontsize=10, color='red', ha='left', va='center', rotation=45)\n",
    "\n",
    "    plt.title(f'Weight of Evidence da variável \"{feature}\"', fontsize=14)\n",
    "    plt.xlabel('Classes', fontsize=14)\n",
    "    plt.ylabel('Weight of Evidence', fontsize=14)\n",
    "    plt.xticks(ha='right', fontsize=10, rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iv(df, feature, target):\n",
    "    bad = df.loc[df[target] == 1].groupby(feature, as_index = False)[target].count().rename({target:'bad'}, axis = 1)\n",
    "    good = df.loc[df[target] == 0].groupby(feature, as_index = False)[target].count().rename({target:'good'}, axis = 1)\n",
    "    \n",
    "    woe = good.merge(bad, on = feature, how = 'left')\n",
    "    woe['percent_bad'] = woe['bad']/woe['bad'].sum()\n",
    "    woe['percent_good'] = woe['good']/woe['good'].sum()\n",
    "    woe['woe'] = round(np.log(woe['percent_bad']/woe['percent_good']), 3)\n",
    "    woe.sort_values(by = 'woe', ascending = True, inplace = True)\n",
    "    woe['iv'] = ((woe['percent_bad'] - woe['percent_good'])*np.log(woe['percent_bad']/woe['percent_good'])).sum()\n",
    "\n",
    "    woe['woe'].fillna(0, inplace = True)\n",
    "    woe['iv'].fillna(0, inplace = True)\n",
    "\n",
    "    weight_of_evidence = woe['woe'].unique()\n",
    "    iv = round(woe['iv'].max(), 2)\n",
    "\n",
    "    dicionario = {feature:iv}\n",
    "\n",
    "    iv_df = pd.DataFrame(list(dicionario.items()), columns=['Feature', 'IV'])\n",
    "    \n",
    "    return iv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separa_feature_target(target, dados):\n",
    "    x = dados.drop(target, axis = 1)\n",
    "    y = dados[[target]]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def train_min_max_scaler(df):\n",
    "\n",
    "    cols = list(df.drop(['id_registro', 'aprovado', 'mau'], axis = 1).columns)\n",
    "\n",
    "    df_scaler = df[cols].copy()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df_scaler)\n",
    "    joblib.dump(scaler, \"../models/scaler.pkl\")\n",
    "    print('Scaler Treinado e Salvo com sucesso!')\n",
    "\n",
    "def Classificador(classificador, x_train, y_train, x_test, y_test, class_weight):\n",
    "\n",
    "    # Puxa o Scaler Treinado com os dados de Treino\n",
    "    scaler = joblib.load(\"../models/scaler.pkl\")\n",
    "    \n",
    "    cols = list(x_train.drop(['id_registro', 'aprovado'], axis = 1).columns)\n",
    "\n",
    "    x_train = x_train[cols]\n",
    "    x_test = x_test[cols]\n",
    "\n",
    "    # Define as colunas categóricas e numéricas\n",
    "    models = {\n",
    "        'Regressão Logística': make_pipeline(\n",
    "            ColumnTransformer([\n",
    "                ('scaler', make_pipeline(scaler), cols)\n",
    "            ]),\n",
    "            LogisticRegression(\n",
    "                random_state=42, # Semente aleatória para reproducibilidade dos resultados\n",
    "                class_weight={0: 1, 1: class_weight}, # Peso atribuído às classes. Pode ser útil para lidar com conjuntos de dados desbalanceados.\n",
    "                C=1, # Parâmetro de regularização inversa. Controla a força da regularização.\n",
    "                penalty='l2', # Tipo de regularização. 'l1', 'l2', 'elasticnet', ou 'none'.\n",
    "                max_iter=50, # Número máximo de iterações para a convergência do otimizador.\n",
    "                solver='liblinear' # Algoritmo de otimização. 'newton-cg', 'lbfgs', 'liblinear' (gradiente descendente), 'sag' (Stochastic gradient descent), 'saga' (Stochastic gradient descent que suporta reg L1).\n",
    "                )\n",
    "        )\n",
    "    }\n",
    "\n",
    "    if classificador in models:\n",
    "        model = models[classificador]\n",
    "    else:\n",
    "        print('Utilize Regressão Logística, Random Forest ou XGBoost como opções de Classificadores!')\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_test = model.predict(x_test)\n",
    "\n",
    "    y_proba_train = model.predict_proba(x_train)\n",
    "    y_proba_test = model.predict_proba(x_test)\n",
    "\n",
    "    return model, y_pred_train, y_pred_test, y_proba_train, y_proba_test\n",
    "\n",
    "def metricas_classificacao(classificador, y_train, y_predict_train, y_test, y_predict_test, y_predict_proba_train, y_predict_proba_test, etapa_1, etapa_2):\n",
    "\n",
    "    predict_proba_train = pd.DataFrame(y_predict_proba_train.tolist(), columns=['predict_proba_0', 'predict_proba_1'])\n",
    "    predict_proba_test = pd.DataFrame(y_predict_proba_test.tolist(), columns=['predict_proba_0', 'predict_proba_1'])\n",
    "\n",
    "    # Treino\n",
    "    accuracy_train = accuracy_score(y_train, y_predict_train)\n",
    "    precision_train = precision_score(y_train, y_predict_train)\n",
    "    recall_train = recall_score(y_train, y_predict_train)\n",
    "    f1_train = f1_score(y_train, y_predict_train)\n",
    "    roc_auc_train = roc_auc_score(y_train['mau'], predict_proba_train['predict_proba_1'])\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train['mau'], predict_proba_train['predict_proba_1'])\n",
    "    ks_train = max(tpr_train - fpr_train)\n",
    "    logloss_train = log_loss(y_train['mau'], predict_proba_train['predict_proba_1'])\n",
    "    metricas_treino = pd.DataFrame(\n",
    "        {\n",
    "            'Acuracia': accuracy_train, \n",
    "            'Precisao': precision_train, \n",
    "            'Recall': recall_train, \n",
    "            'F1-Score': f1_train, \n",
    "            'AUC': roc_auc_train, \n",
    "            'KS': ks_train, \n",
    "            'LogLoss':logloss_train,\n",
    "            'Etapa': etapa_1, \n",
    "            'Classificador': classificador\n",
    "        }, \n",
    "        index=[0]\n",
    "    )\n",
    "    \n",
    "    # Teste\n",
    "    accuracy_test = accuracy_score(y_test, y_predict_test)\n",
    "    precision_test = precision_score(y_test, y_predict_test)\n",
    "    recall_test = recall_score(y_test, y_predict_test)\n",
    "    f1_test = f1_score(y_test, y_predict_test)\n",
    "    roc_auc_test = roc_auc_score(y_test['mau'], predict_proba_test['predict_proba_1'])\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test['mau'], predict_proba_test['predict_proba_1'])\n",
    "    ks_test = max(tpr_test - fpr_test)\n",
    "    logloss_test = log_loss(y_test['mau'], predict_proba_test['predict_proba_1'])\n",
    "    metricas_teste = pd.DataFrame(\n",
    "        {\n",
    "            'Acuracia': accuracy_test, \n",
    "            'Precisao': precision_test, \n",
    "            'Recall': recall_test, \n",
    "            'F1-Score': f1_test, \n",
    "            'AUC': roc_auc_test, \n",
    "            'KS': ks_test, \n",
    "            'LogLoss':logloss_test,\n",
    "            'Etapa': etapa_2, \n",
    "            'Classificador': classificador\n",
    "        }, \n",
    "        index=[0]\n",
    "    )\n",
    "    \n",
    "    # Consolidando\n",
    "    metricas_finais = pd.concat([metricas_treino, metricas_teste])\n",
    "\n",
    "    return metricas_finais\n",
    "\n",
    "\n",
    "def metricas_classificacao_modelos_juntos(lista_modelos):\n",
    "    if len(lista_modelos) > 0:\n",
    "        metricas_modelos = pd.concat(lista_modelos)#.set_index('Classificador')\n",
    "    else:\n",
    "        metricas_modelos = lista_modelos[0]\n",
    "    # Redefina o índice para torná-lo exclusivo\n",
    "    df = metricas_modelos.reset_index(drop=True)\n",
    "    df = df.round(2)\n",
    "\n",
    "    # Função para formatar as células com base na Etapa\n",
    "    def color_etapa(val):\n",
    "        color = 'black'\n",
    "        if val == 'treino':\n",
    "            color = 'blue'\n",
    "        elif val == 'teste':\n",
    "            color = 'red'\n",
    "        return f'color: {color}; font-weight: bold;'\n",
    "\n",
    "    # Função para formatar os valores com até duas casas decimais\n",
    "    def format_values(val):\n",
    "        if isinstance(val, (int, float)):\n",
    "            return f'{val:.2f}'\n",
    "        return val\n",
    "\n",
    "    # Estilizando o DataFrame\n",
    "    styled_df = df.style\\\n",
    "        .format(format_values)\\\n",
    "        .applymap(lambda x: 'color: black; font-weight: bold; background-color: white; font-size: 14px', subset=pd.IndexSlice[:, :])\\\n",
    "        .applymap(color_etapa, subset=pd.IndexSlice[:, :])\\\n",
    "        .applymap(lambda x: 'color: black; font-weight: bold; background-color: #white; font-size: 14px', subset=pd.IndexSlice[:, 'Acuracia':'F1-Score'])\\\n",
    "        .applymap(lambda x: 'color: black; font-weight: bold; background-color: #white; font-size: 14px', subset=pd.IndexSlice[:, 'Etapa'])\\\n",
    "        .set_table_styles([\n",
    "            {'selector': 'thead', 'props': [('color', 'black'), ('font-weight', 'bold'), ('background-color', 'lightgray')]}\n",
    "        ])\n",
    "\n",
    "    # Mostrando o DataFrame estilizado\n",
    "    styled_df\n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red' style='font-size: 40px;'> 1. Leitura do Dataset </font>\n",
    "<hr style='border: 2px solid red;'>\n",
    "\n",
    "https://www.kaggle.com/datasets/faizaniftikharjanjua/metaverse-financial-transactions-dataset\n",
    "\n",
    "### 1) Descrição\n",
    "\n",
    "- Timestamp: Date and time of the transaction.\n",
    "- Hour of Day: Hour part of the transaction timestamp.\n",
    "- Sending Address: Blockchain address of the sender.\n",
    "- Receiving Address: Blockchain address of the receiver.\n",
    "- Amount: Transaction amount in a simulated currency.\n",
    "- Transaction Type: Categorization of the transaction (e.g., transfer, sale, purchase, scam, phishing).\n",
    "- Location Region: Simulated geographical region of the transaction.\n",
    "- IP Prefix: Simulated IP address prefix for the transaction.\n",
    "- Login Frequency: Frequency of login sessions by the user, varying by age group.\n",
    "- Session Duration: Duration of activity sessions in minutes.\n",
    "- Purchase Pattern: Behavioral pattern of purchases (e.g., focused, random, high-value).\n",
    "- Age Group: Categorization of users into new, established, and veteran based on their activity history.\n",
    "- Risk Score: Calculated risk score based on transaction characteristics and user behavior.\n",
    "- Anomaly: Risk level assessment (e.g., high_risk, moderate_risk, low_risk).\n",
    "\n",
    "### 2) Desafios\n",
    "\n",
    "- Espera-se desenvolver variáveis utilizando grafos, de modo a marcar os clientes suspeitos a partir dessas variáveis\n",
    "- Colocar essas e demais variáveis dentro de um classificador e criar um score de suspeita, de modo a comparar os resultados puros com as variáveis de grafos vs resultados do modelo\n",
    "- Identificar oportunidades para melhorias usando técnicas de detecção de anomalias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metaverse = pd.read_csv('../data/metaverse_transactions_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>sending_address</th>\n",
       "      <th>receiving_address</th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>location_region</th>\n",
       "      <th>ip_prefix</th>\n",
       "      <th>login_frequency</th>\n",
       "      <th>session_duration</th>\n",
       "      <th>purchase_pattern</th>\n",
       "      <th>age_group</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-11 12:47:27</td>\n",
       "      <td>12</td>\n",
       "      <td>0x9d32d0bf2c00f41ce7ca01b66e174cc4dcb0c1da</td>\n",
       "      <td>0x39f82e1c09bc6d7baccc1e79e5621ff812f50572</td>\n",
       "      <td>796.95</td>\n",
       "      <td>transfer</td>\n",
       "      <td>Europe</td>\n",
       "      <td>192.00</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>focused</td>\n",
       "      <td>established</td>\n",
       "      <td>18.75</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-14 19:12:46</td>\n",
       "      <td>19</td>\n",
       "      <td>0xd6e251c23cbf52dbd472f079147873e655d8096f</td>\n",
       "      <td>0x51e8fbe24f124e0e30a614e14401b9bbfed5384c</td>\n",
       "      <td>0.01</td>\n",
       "      <td>purchase</td>\n",
       "      <td>South America</td>\n",
       "      <td>172.00</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "      <td>focused</td>\n",
       "      <td>established</td>\n",
       "      <td>25.00</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-18 16:26:59</td>\n",
       "      <td>16</td>\n",
       "      <td>0x2e0925b922fed01f6a85d213ae2718f54b8ca305</td>\n",
       "      <td>0x52c7911879f783d590af45bda0c0ef2b8536706f</td>\n",
       "      <td>778.20</td>\n",
       "      <td>purchase</td>\n",
       "      <td>Asia</td>\n",
       "      <td>192.17</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>focused</td>\n",
       "      <td>established</td>\n",
       "      <td>31.25</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-15 09:20:04</td>\n",
       "      <td>9</td>\n",
       "      <td>0x93efefc25fcaf31d7695f28018d7a11ece55457f</td>\n",
       "      <td>0x8ac3b7bd531b3a833032f07d4e47c7af6ea7bace</td>\n",
       "      <td>300.84</td>\n",
       "      <td>transfer</td>\n",
       "      <td>South America</td>\n",
       "      <td>172.00</td>\n",
       "      <td>8</td>\n",
       "      <td>111</td>\n",
       "      <td>high_value</td>\n",
       "      <td>veteran</td>\n",
       "      <td>36.75</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-18 14:35:30</td>\n",
       "      <td>14</td>\n",
       "      <td>0xad3b8de45d63f5cce28aef9a82cf30c397c6ceb9</td>\n",
       "      <td>0x6fdc047c2391615b3facd79b4588c7e9106e49f2</td>\n",
       "      <td>775.57</td>\n",
       "      <td>sale</td>\n",
       "      <td>Africa</td>\n",
       "      <td>172.16</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>high_value</td>\n",
       "      <td>veteran</td>\n",
       "      <td>62.50</td>\n",
       "      <td>moderate_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78595</th>\n",
       "      <td>2022-02-13 12:52:27</td>\n",
       "      <td>12</td>\n",
       "      <td>0xa99b9a7f5c5dd37429771efd3b93c6fbe1ab2936</td>\n",
       "      <td>0x5a78c88c5fc1e9b512f6c64e266b46a9db0a7238</td>\n",
       "      <td>660.28</td>\n",
       "      <td>transfer</td>\n",
       "      <td>Africa</td>\n",
       "      <td>172.00</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>random</td>\n",
       "      <td>new</td>\n",
       "      <td>26.25</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78596</th>\n",
       "      <td>2022-02-16 16:15:29</td>\n",
       "      <td>16</td>\n",
       "      <td>0xcca095ad2d508c200bda1141f783d77c3e6cbb08</td>\n",
       "      <td>0xb71a4df291ea8e9996a74e77ab63f5abcbfe90f5</td>\n",
       "      <td>310.27</td>\n",
       "      <td>purchase</td>\n",
       "      <td>Africa</td>\n",
       "      <td>172.00</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>focused</td>\n",
       "      <td>established</td>\n",
       "      <td>26.25</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78597</th>\n",
       "      <td>2022-08-04 16:06:42</td>\n",
       "      <td>16</td>\n",
       "      <td>0xe0b340171486b08f0c1f4ac607ed776478cefd2b</td>\n",
       "      <td>0x577754308538f4be10a41afb4f8900cd24d7098f</td>\n",
       "      <td>624.67</td>\n",
       "      <td>purchase</td>\n",
       "      <td>Africa</td>\n",
       "      <td>192.00</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>random</td>\n",
       "      <td>new</td>\n",
       "      <td>36.75</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78598</th>\n",
       "      <td>2022-02-26 04:06:08</td>\n",
       "      <td>4</td>\n",
       "      <td>0xb0b2f6fc707fbb7f9d27a9f4fe0cb0d6b39a0155</td>\n",
       "      <td>0xd6ba299fdd52f09f01d9648036ca446498c01ac2</td>\n",
       "      <td>401.39</td>\n",
       "      <td>purchase</td>\n",
       "      <td>Asia</td>\n",
       "      <td>192.17</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>focused</td>\n",
       "      <td>established</td>\n",
       "      <td>35.44</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78599</th>\n",
       "      <td>2022-06-25 14:27:37</td>\n",
       "      <td>14</td>\n",
       "      <td>0x6ea0e02fb6ee893dc3b70b98df1a48165d28eb09</td>\n",
       "      <td>0xc28cbdb253f12174f7aa80ff6c6660f2e09397d7</td>\n",
       "      <td>523.95</td>\n",
       "      <td>transfer</td>\n",
       "      <td>North America</td>\n",
       "      <td>172.00</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>focused</td>\n",
       "      <td>established</td>\n",
       "      <td>15.75</td>\n",
       "      <td>low_risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78600 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp  hour_of_day                             sending_address                           receiving_address  amount transaction_type location_region  ip_prefix  login_frequency  session_duration purchase_pattern    age_group  risk_score        anomaly\n",
       "0      2022-04-11 12:47:27           12  0x9d32d0bf2c00f41ce7ca01b66e174cc4dcb0c1da  0x39f82e1c09bc6d7baccc1e79e5621ff812f50572  796.95         transfer          Europe     192.00                3                48          focused  established       18.75       low_risk\n",
       "1      2022-06-14 19:12:46           19  0xd6e251c23cbf52dbd472f079147873e655d8096f  0x51e8fbe24f124e0e30a614e14401b9bbfed5384c    0.01         purchase   South America     172.00                5                61          focused  established       25.00       low_risk\n",
       "2      2022-01-18 16:26:59           16  0x2e0925b922fed01f6a85d213ae2718f54b8ca305  0x52c7911879f783d590af45bda0c0ef2b8536706f  778.20         purchase            Asia     192.17                3                74          focused  established       31.25       low_risk\n",
       "3      2022-06-15 09:20:04            9  0x93efefc25fcaf31d7695f28018d7a11ece55457f  0x8ac3b7bd531b3a833032f07d4e47c7af6ea7bace  300.84         transfer   South America     172.00                8               111       high_value      veteran       36.75       low_risk\n",
       "4      2022-02-18 14:35:30           14  0xad3b8de45d63f5cce28aef9a82cf30c397c6ceb9  0x6fdc047c2391615b3facd79b4588c7e9106e49f2  775.57             sale          Africa     172.16                6               100       high_value      veteran       62.50  moderate_risk\n",
       "...                    ...          ...                                         ...                                         ...     ...              ...             ...        ...              ...               ...              ...          ...         ...            ...\n",
       "78595  2022-02-13 12:52:27           12  0xa99b9a7f5c5dd37429771efd3b93c6fbe1ab2936  0x5a78c88c5fc1e9b512f6c64e266b46a9db0a7238  660.28         transfer          Africa     172.00                1                27           random          new       26.25       low_risk\n",
       "78596  2022-02-16 16:15:29           16  0xcca095ad2d508c200bda1141f783d77c3e6cbb08  0xb71a4df291ea8e9996a74e77ab63f5abcbfe90f5  310.27         purchase          Africa     172.00                5                60          focused  established       26.25       low_risk\n",
       "78597  2022-08-04 16:06:42           16  0xe0b340171486b08f0c1f4ac607ed776478cefd2b  0x577754308538f4be10a41afb4f8900cd24d7098f  624.67         purchase          Africa     192.00                1                34           random          new       36.75       low_risk\n",
       "78598  2022-02-26 04:06:08            4  0xb0b2f6fc707fbb7f9d27a9f4fe0cb0d6b39a0155  0xd6ba299fdd52f09f01d9648036ca446498c01ac2  401.39         purchase            Asia     192.17                4                56          focused  established       35.44       low_risk\n",
       "78599  2022-06-25 14:27:37           14  0x6ea0e02fb6ee893dc3b70b98df1a48165d28eb09  0xc28cbdb253f12174f7aa80ff6c6660f2e09397d7  523.95         transfer   North America     172.00                4                56          focused  established       15.75       low_risk\n",
       "\n",
       "[78600 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metaverse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a9a036f549c34521959f2e35c11282d59c2fa6997060d671aa6f29f1c44b680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
